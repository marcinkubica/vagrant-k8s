16:00 $ vagrant up
Bringing machine 'control' up with 'virtualbox' provider...
Bringing machine 'node-01' up with 'virtualbox' provider...
Bringing machine 'node-02' up with 'virtualbox' provider...
==> control: Importing base box 'ubuntu/focal64'...
==> control: Matching MAC address for NAT networking...
==> control: Checking if box 'ubuntu/focal64' version '20210610.0.0' is up to date...
==> control: Setting the name of the VM: vagrant-k8s_control_1624114870830_35750
==> control: Clearing any previously set network interfaces...
==> control: Preparing network interfaces based on configuration...
    control: Adapter 1: nat
    control: Adapter 2: hostonly
==> control: Forwarding ports...
    control: 22 (guest) => 60200 (host) (adapter 1)
    control: 6443 (guest) => 6443 (host) (adapter 1)
    control: 22 (guest) => 2222 (host) (adapter 1)
==> control: Running 'pre-boot' VM customizations...
==> control: Booting VM...
==> control: Waiting for machine to boot. This may take a few minutes...
    control: SSH address: 127.0.0.1:2222
    control: SSH username: vagrant
    control: SSH auth method: private key
    control:
    control: Vagrant insecure key detected. Vagrant will automatically replace
    control: this with a newly generated keypair for better security.
    control:
    control: Inserting generated public key within guest...
    control: Removing insecure key from the guest if it's present...
    control: Key inserted! Disconnecting and reconnecting using new SSH key...
==> control: Machine booted and ready!
==> control: Checking for guest additions in VM...
==> control: Setting hostname...
==> control: Configuring and enabling network interfaces...
==> control: Mounting shared folders...
    control: /vagrant => /home/booker/Projects/vagrant-k8s
==> node-01: Importing base box 'ubuntu/focal64'...
==> node-01: Matching MAC address for NAT networking...
==> node-01: Checking if box 'ubuntu/focal64' version '20210610.0.0' is up to date...
==> node-01: Setting the name of the VM: vagrant-k8s_node-01_1624114912577_34828
==> node-01: Fixed port collision for 22 => 2222. Now on port 2200.
==> node-01: Clearing any previously set network interfaces...
==> node-01: Preparing network interfaces based on configuration...
    node-01: Adapter 1: nat
    node-01: Adapter 2: hostonly
==> node-01: Forwarding ports...
    node-01: 22 (guest) => 60201 (host) (adapter 1)
    node-01: 22 (guest) => 2200 (host) (adapter 1)
==> node-01: Running 'pre-boot' VM customizations...
==> node-01: Booting VM...
==> node-01: Waiting for machine to boot. This may take a few minutes...
    node-01: SSH address: 127.0.0.1:2200
    node-01: SSH username: vagrant
    node-01: SSH auth method: private key
    node-01: Warning: Connection reset. Retrying...
    node-01: Warning: Remote connection disconnect. Retrying...
    node-01:
    node-01: Vagrant insecure key detected. Vagrant will automatically replace
    node-01: this with a newly generated keypair for better security.
    node-01:
    node-01: Inserting generated public key within guest...
    node-01: Removing insecure key from the guest if it's present...
    node-01: Key inserted! Disconnecting and reconnecting using new SSH key...
==> node-01: Machine booted and ready!
==> node-01: Checking for guest additions in VM...
==> node-01: Setting hostname...
==> node-01: Configuring and enabling network interfaces...
==> node-01: Mounting shared folders...
    node-01: /vagrant => /home/booker/Projects/vagrant-k8s
==> node-02: Importing base box 'ubuntu/focal64'...
==> node-02: Matching MAC address for NAT networking...
==> node-02: Checking if box 'ubuntu/focal64' version '20210610.0.0' is up to date...
==> node-02: Setting the name of the VM: vagrant-k8s_node-02_1624114956940_45822
==> node-02: Fixed port collision for 22 => 2222. Now on port 2201.
==> node-02: Clearing any previously set network interfaces...
==> node-02: Preparing network interfaces based on configuration...
    node-02: Adapter 1: nat
    node-02: Adapter 2: hostonly
==> node-02: Forwarding ports...
    node-02: 22 (guest) => 60202 (host) (adapter 1)
    node-02: 22 (guest) => 2201 (host) (adapter 1)
==> node-02: Running 'pre-boot' VM customizations...
==> node-02: Booting VM...
==> node-02: Waiting for machine to boot. This may take a few minutes...
    node-02: SSH address: 127.0.0.1:2201
    node-02: SSH username: vagrant
    node-02: SSH auth method: private key
    node-02: Warning: Connection reset. Retrying...
    node-02: Warning: Remote connection disconnect. Retrying...
    node-02:
    node-02: Vagrant insecure key detected. Vagrant will automatically replace
    node-02: this with a newly generated keypair for better security.
    node-02:
    node-02: Inserting generated public key within guest...
    node-02: Removing insecure key from the guest if it's present...
    node-02: Key inserted! Disconnecting and reconnecting using new SSH key...
==> node-02: Machine booted and ready!
==> node-02: Checking for guest additions in VM...
==> node-02: Setting hostname...
==> node-02: Configuring and enabling network interfaces...
==> node-02: Mounting shared folders...
    node-02: /vagrant => /home/booker/Projects/vagrant-k8s
==> node-02: Running action triggers after up ...
==> node-02: Running trigger: ansible trigger...
==> node-02: Attempting to run ./vagrant-k8s.sh
    node-02: Running local script: vagrant-k8s.sh
    node-02:
    node-02: PLAY [all] *********************************************************************
    node-02:
    node-02:
    node-02: TASK [Gathering Facts] *********************************************************
    node-02: Saturday 19 June 2021  16:03:05 +0100 (0:00:00.029)       0:00:00.029 *********
    node-02:
    node-02: ok: [node-02]
    node-02:
    node-02: ok: [control]
    node-02:
    node-02: ok: [node-01]
    node-02:
    node-02:
    node-02: TASK [k8s-prep : create containerd.conf] ***************************************
    node-02: Saturday 19 June 2021  16:03:08 +0100 (0:00:03.236)       0:00:03.265 *********
    node-02:
    node-02: changed: [control]
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-prep : insert containerd modules] ************************************
    node-02:
    node-02: Saturday 19 June 2021  16:03:09 +0100 (0:00:00.777)       0:00:04.042 *********
    node-02:
    node-02: changed: [node-02] => (item=overlay)
    node-02:
    node-02: changed: [control] => (item=overlay)
    node-02:
    node-02: changed: [node-01] => (item=overlay)
    node-02:
    node-02: changed: [control] => (item=br_netfilter)
    node-02:
    node-02: changed: [node-02] => (item=br_netfilter)
    node-02:
    node-02: changed: [node-01] => (item=br_netfilter)
    node-02:
    node-02:
    node-02: TASK [k8s-prep : create kubernetes-cri sysctl] *********************************
    node-02:
    node-02: Saturday 19 June 2021  16:03:10 +0100 (0:00:00.795)       0:00:04.837 *********
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [control]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-prep : reload sysctl] ************************************************
    node-02:
    node-02: Saturday 19 June 2021  16:03:11 +0100 (0:00:00.680)       0:00:05.518 *********
    node-02:
    node-02: changed: [control]
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-prep : install containerd] *******************************************
    node-02:
    node-02: Saturday 19 June 2021  16:03:11 +0100 (0:00:00.419)       0:00:05.938 *********
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [control]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-prep : grab default containerd config] *******************************
    node-02:
    node-02: Saturday 19 June 2021  16:04:08 +0100 (0:00:57.034)       0:01:02.973 *********
    node-02:
    node-02: changed: [control]
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-prep : create /etc/containerd] ***************************************
    node-02:
    node-02: Saturday 19 June 2021  16:04:08 +0100 (0:00:00.399)       0:01:03.372 *********
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02: changed: [control]
    node-02:
    node-02:
    node-02: TASK [k8s-prep : generate containerd config] ***********************************
    node-02:
    node-02: Saturday 19 June 2021  16:04:09 +0100 (0:00:00.492)       0:01:03.864 *********
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [control]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-prep : restart containerd] *******************************************
    node-02:
    node-02: Saturday 19 June 2021  16:04:10 +0100 (0:00:00.648)       0:01:04.513 *********
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02: changed: [control]
    node-02:
    node-02:
    node-02: TASK [k8s-prep : check containerd] *********************************************
    node-02:
    node-02: Saturday 19 June 2021  16:04:10 +0100 (0:00:00.821)       0:01:05.334 *********
    node-02:
    node-02: ok: [control]
    node-02:
    node-02: ok: [node-01]
    node-02:
    node-02: ok: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-prep : disable swap] *************************************************
    node-02:
    node-02: Saturday 19 June 2021  16:04:11 +0100 (0:00:00.494)       0:01:05.828 *********
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02: changed: [control]
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02:
    node-02: TASK [k8s-prep : comment out swap entries in /etc/fstab] ***********************
    node-02:
    node-02: Saturday 19 June 2021  16:04:11 +0100 (0:00:00.348)       0:01:06.177 *********
    node-02:
    node-02: ok: [control]
    node-02:
    node-02: ok: [node-01]
    node-02:
    node-02: ok: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-install : install deps for k8s] **************************************
    node-02:
    node-02: Saturday 19 June 2021  16:04:12 +0100 (0:00:00.411)       0:01:06.589 *********
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [control]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-install : add k8s repo gpg key] **************************************
    node-02:
    node-02: Saturday 19 June 2021  16:04:17 +0100 (0:00:05.662)       0:01:12.251 *********
    node-02:
    node-02: changed: [control]
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-install : add k8s repository] ****************************************
    node-02:
    node-02: Saturday 19 June 2021  16:04:19 +0100 (0:00:01.453)       0:01:13.705 *********
    node-02:
    node-02: changed: [control]
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-install : create /ect/default/kubelet to setup INTERNAL-IP for a node] ***
    node-02:
    node-02: Saturday 19 June 2021  16:04:32 +0100 (0:00:12.748)       0:01:26.453 *********
    node-02:
    node-02: changed: [control]
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-install : install k8s components - kubelet, kubeadm, kubectl 1.20.1-00] ***
    node-02:
    node-02: Saturday 19 June 2021  16:04:32 +0100 (0:00:00.752)       0:01:27.205 *********
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [control]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-install : mark hold k8s packages] ************************************
    node-02:
    node-02: Saturday 19 June 2021  16:05:33 +0100 (0:01:00.338)       0:02:27.544 *********
    node-02:
    node-02: changed: [node-01] => (item=kubelet)
    node-02:
    node-02: changed: [control] => (item=kubelet)
    node-02:
    node-02: changed: [node-02] => (item=kubelet)
    node-02:
    node-02: changed: [node-01] => (item=kubeadm)
    node-02:
    node-02: changed: [node-02] => (item=kubeadm)
    node-02:
    node-02: changed: [control] => (item=kubeadm)
    node-02:
    node-02: changed: [node-01] => (item=kubectl)
    node-02:
    node-02: changed: [node-02] => (item=kubectl)
    node-02:
    node-02: changed: [control] => (item=kubectl)
    node-02:
    node-02:
    node-02: PLAY [control] *****************************************************************
    node-02:
    node-02:
    node-02: TASK [Gathering Facts] *********************************************************
    node-02:
    node-02: Saturday 19 June 2021  16:05:34 +0100 (0:00:01.176)       0:02:28.720 *********
    node-02:
    node-02: ok: [control]
    node-02:
    node-02:
    node-02: TASK [k8s-init : pull k8s images] **********************************************
    node-02:
    node-02: Saturday 19 June 2021  16:05:35 +0100 (0:00:01.016)       0:02:29.737 *********
    node-02:
    node-02: changed: [control]
    node-02:
    node-02:
    node-02: TASK [k8s-init : debug] ********************************************************
    node-02:
    node-02: Saturday 19 June 2021  16:06:34 +0100 (0:00:59.495)       0:03:29.232 *********
    node-02:
    node-02: ok: [control] => {
    node-02:     "pull": {
    node-02:         "changed": true,
    node-02:         "cmd": "kubeadm config images pull",
    node-02:         "delta": "0:00:59.229503",
    node-02:         "end": "2021-06-19 15:06:33.449661",
    node-02:         "failed": false,
    node-02:         "rc": 0,
    node-02:         "start": "2021-06-19 15:05:34.220158",
    node-02:         "stderr": "I0619 15:05:34.735055    5703 version.go:251] remote version is much newer: v1.21.2; falling back to: stable-1.20",
    node-02:         "stderr_lines": [
    node-02:             "I0619 15:05:34.735055    5703 version.go:251] remote version is much newer: v1.21.2; falling back to: stable-1.20"
    node-02:         ],
    node-02:         "stdout": "[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.20.8\n[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.20.8\n[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.20.8\n[config/images] Pulled k8s.gcr.io/kube-proxy:v1.20.8\n[config/images] Pulled k8s.gcr.io/pause:3.2\n[config/images] Pulled k8s.gcr.io/etcd:3.4.13-0\n[config/images] Pulled k8s.gcr.io/coredns:1.7.0",
    node-02:         "stdout_lines": [
    node-02:             "[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.20.8",
    node-02:             "[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.20.8",
    node-02:             "[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.20.8",
    node-02:             "[config/images] Pulled k8s.gcr.io/kube-proxy:v1.20.8",
    node-02:             "[config/images] Pulled k8s.gcr.io/pause:3.2",
    node-02:             "[config/images] Pulled k8s.gcr.io/etcd:3.4.13-0",
    node-02:             "[config/images] Pulled k8s.gcr.io/coredns:1.7.0"
    node-02:         ]
    node-02:     }
    node-02: }
    node-02:
    node-02:
    node-02: TASK [k8s-init : initialise kubernetes cluster] ********************************
    node-02:
    node-02: Saturday 19 June 2021  16:06:34 +0100 (0:00:00.024)       0:03:29.257 *********
    node-02:
    node-02: changed: [control]
    node-02:
    node-02:
    node-02: TASK [k8s-init : debug] ********************************************************
    node-02:
    node-02: Saturday 19 June 2021  16:07:05 +0100 (0:00:30.317)       0:03:59.574 *********
    node-02:
    node-02: ok: [control] => {
    node-02:     "init": {
    node-02:         "changed": true,
    node-02:         "cmd": "kubeadm init --apiserver-advertise-address 10.0.0.9 --pod-network-cidr 192.168.0.0/16",
    node-02:         "delta": "0:00:29.475373",
    node-02:         "end": "2021-06-19 15:07:03.807056",
    node-02:         "failed": false,
    node-02:         "rc": 0,
    node-02:         "start": "2021-06-19 15:06:34.331683",
    node-02:         "stderr": "I0619 15:06:34.746894    5971 version.go:251] remote version is much newer: v1.21.2; falling back to: stable-1.20",
    node-02:         "stderr_lines": [
    node-02:             "I0619 15:06:34.746894    5971 version.go:251] remote version is much newer: v1.21.2; falling back to: stable-1.20"
    node-02:         ],
    node-02:         "stdout": "[init] Using Kubernetes version: v1.20.8\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [control kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.0.0.9]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [control localhost] and IPs [10.0.0.9 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [control localhost] and IPs [10.0.0.9 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\n[apiclient] All control plane components are healthy after 21.003052 seconds\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config-1.20\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Skipping phase. Please see --upload-certs\n[mark-control-plane] Marking the node control as control-plane by adding the labels \"node-role.kubernetes.io/master=''\" and \"node-role.kubernetes.io/control-plane='' (deprecated)\"\n[mark-control-plane] Marking the node control as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n[bootstrap-token] Using token: gpwzok.hwwk0fzu1858dhhm\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 10.0.0.9:6443 --token gpwzok.hwwk0fzu1858dhhm \\\n    --discovery-token-ca-cert-hash sha256:8dbc28542c14f704b82eda584ca168ec88605131ca121d9a44415bf4e0b329da ",
    node-02:         "stdout_lines": [
    node-02:             "[init] Using Kubernetes version: v1.20.8",
    node-02:             "[preflight] Running pre-flight checks",
    node-02:             "[preflight] Pulling images required for setting up a Kubernetes cluster",
    node-02:             "[preflight] This might take a minute or two, depending on the speed of your internet connection",
    node-02:             "[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'",
    node-02:             "[certs] Using certificateDir folder \"/etc/kubernetes/pki\"",
    node-02:             "[certs] Generating \"ca\" certificate and key",
    node-02:             "[certs] Generating \"apiserver\" certificate and key",
    node-02:             "[certs] apiserver serving cert is signed for DNS names [control kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.0.0.9]",
    node-02:             "[certs] Generating \"apiserver-kubelet-client\" certificate and key",
    node-02:             "[certs] Generating \"front-proxy-ca\" certificate and key",
    node-02:             "[certs] Generating \"front-proxy-client\" certificate and key",
    node-02:             "[certs] Generating \"etcd/ca\" certificate and key",
    node-02:             "[certs] Generating \"etcd/server\" certificate and key",
    node-02:             "[certs] etcd/server serving cert is signed for DNS names [control localhost] and IPs [10.0.0.9 127.0.0.1 ::1]",
    node-02:             "[certs] Generating \"etcd/peer\" certificate and key",
    node-02:             "[certs] etcd/peer serving cert is signed for DNS names [control localhost] and IPs [10.0.0.9 127.0.0.1 ::1]",
    node-02:             "[certs] Generating \"etcd/healthcheck-client\" certificate and key",
    node-02:             "[certs] Generating \"apiserver-etcd-client\" certificate and key",
    node-02:             "[certs] Generating \"sa\" key and public key",
    node-02:             "[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"",
    node-02:             "[kubeconfig] Writing \"admin.conf\" kubeconfig file",
    node-02:             "[kubeconfig] Writing \"kubelet.conf\" kubeconfig file",
    node-02:             "[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file",
    node-02:             "[kubeconfig] Writing \"scheduler.conf\" kubeconfig file",
    node-02:             "[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"",
    node-02:             "[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"",
    node-02:             "[kubelet-start] Starting the kubelet",
    node-02:             "[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"",
    node-02:             "[control-plane] Creating static Pod manifest for \"kube-apiserver\"",
    node-02:             "[control-plane] Creating static Pod manifest for \"kube-controller-manager\"",
    node-02:             "[control-plane] Creating static Pod manifest for \"kube-scheduler\"",
    node-02:             "[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"",
    node-02:             "[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s",
    node-02:             "[apiclient] All control plane components are healthy after 21.003052 seconds",
    node-02:             "[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace",
    node-02:             "[kubelet] Creating a ConfigMap \"kubelet-config-1.20\" in namespace kube-system with the configuration for the kubelets in the cluster",
    node-02:             "[upload-certs] Skipping phase. Please see --upload-certs",
    node-02:             "[mark-control-plane] Marking the node control as control-plane by adding the labels \"node-role.kubernetes.io/master=''\" and \"node-role.kubernetes.io/control-plane='' (deprecated)\"",
    node-02:             "[mark-control-plane] Marking the node control as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]",
    node-02:             "[bootstrap-token] Using token: gpwzok.hwwk0fzu1858dhhm",
    node-02:             "[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles",
    node-02:             "[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes",
    node-02:             "[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials",
    node-02:             "[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token",
    node-02:             "[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster",
    node-02:             "[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace",
    node-02:             "[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key",
    node-02:             "[addons] Applied essential addon: CoreDNS",
    node-02:             "[addons] Applied essential addon: kube-proxy",
    node-02:             "",
    node-02:             "Your Kubernetes control-plane has initialized successfully!",
    node-02:             "",
    node-02:             "To start using your cluster, you need to run the following as a regular user:",
    node-02:             "",
    node-02:             "  mkdir -p $HOME/.kube",
    node-02:             "  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config",
    node-02:             "  sudo chown $(id -u):$(id -g) $HOME/.kube/config",
    node-02:             "",
    node-02:             "Alternatively, if you are the root user, you can run:",
    node-02:             "",
    node-02:             "  export KUBECONFIG=/etc/kubernetes/admin.conf",
    node-02:             "",
    node-02:             "You should now deploy a pod network to the cluster.",
    node-02:             "Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:",
    node-02:             "  https://kubernetes.io/docs/concepts/cluster-administration/addons/",
    node-02:             "",
    node-02:             "Then you can join any number of worker nodes by running the following on each as root:",
    node-02:             "",
    node-02:             "kubeadm join 10.0.0.9:6443 --token gpwzok.hwwk0fzu1858dhhm \\",
    node-02:             "    --discovery-token-ca-cert-hash sha256:8dbc28542c14f704b82eda584ca168ec88605131ca121d9a44415bf4e0b329da "
    node-02:         ]
    node-02:     }
    node-02: }
    node-02:
    node-02:
    node-02: TASK [k8s-init : setup vagrant user for kube config | directory] ***************
    node-02:
    node-02: Saturday 19 June 2021  16:07:05 +0100 (0:00:00.033)       0:03:59.608 *********
    node-02:
    node-02: changed: [control]
    node-02:
    node-02:
    node-02: TASK [k8s-init : setup vagrant user for kube config | copy admin conf] *********
    node-02:
    node-02: Saturday 19 June 2021  16:07:05 +0100 (0:00:00.451)       0:04:00.060 *********
    node-02:
    node-02: changed: [control]
    node-02:
    node-02:
    node-02: TASK [k8s-network : apply calico networking] ***********************************
    node-02:
    node-02: Saturday 19 June 2021  16:07:05 +0100 (0:00:00.303)       0:04:00.363 *********
    node-02:
    node-02: changed: [control]
    node-02:
    node-02:
    node-02: TASK [k8s-network : debug] *****************************************************
    node-02:
    node-02: Saturday 19 June 2021  16:07:07 +0100 (0:00:01.808)       0:04:02.171 *********
    node-02:
    node-02: ok: [control] => {
    node-02:     "c": {
    node-02:         "changed": true,
    node-02:         "cmd": "kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml",
    node-02:         "delta": "0:00:01.504774",
    node-02:         "end": "2021-06-19 15:07:06.422229",
    node-02:         "failed": false,
    node-02:         "rc": 0,
    node-02:         "start": "2021-06-19 15:07:04.917455",
    node-02:         "stderr": "",
    node-02:         "stderr_lines": [],
    node-02:         "stdout": "configmap/calico-config created\ncustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\nclusterrole.rbac.authorization.k8s.io/calico-kube-controllers created\nclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created\nclusterrole.rbac.authorization.k8s.io/calico-node created\nclusterrolebinding.rbac.authorization.k8s.io/calico-node created\ndaemonset.apps/calico-node created\nserviceaccount/calico-node created\ndeployment.apps/calico-kube-controllers created\nserviceaccount/calico-kube-controllers created\npoddisruptionbudget.policy/calico-kube-controllers created",
    node-02:         "stdout_lines": [
    node-02:             "configmap/calico-config created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created",
    node-02:             "customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created",
    node-02:             "clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created",
    node-02:             "clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created",
    node-02:             "clusterrole.rbac.authorization.k8s.io/calico-node created",
    node-02:             "clusterrolebinding.rbac.authorization.k8s.io/calico-node created",
    node-02:             "daemonset.apps/calico-node created",
    node-02:             "serviceaccount/calico-node created",
    node-02:             "deployment.apps/calico-kube-controllers created",
    node-02:             "serviceaccount/calico-kube-controllers created",
    node-02:             "poddisruptionbudget.policy/calico-kube-controllers created"
    node-02:         ]
    node-02:     }
    node-02: }
    node-02:
    node-02:
    node-02: TASK [k8s-network : check for all pods running] ********************************
    node-02:
    node-02: Saturday 19 June 2021  16:07:07 +0100 (0:00:00.026)       0:04:02.198 *********
    node-02:
    node-02: changed: [control]
    node-02:
    node-02:
    node-02: PLAY [workers] *****************************************************************
    node-02:
    node-02:
    node-02: TASK [Gathering Facts] *********************************************************
    node-02:
    node-02: Saturday 19 June 2021  16:07:08 +0100 (0:00:00.526)       0:04:02.724 *********
    node-02:
    node-02: ok: [node-01]
    node-02:
    node-02: ok: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-worker : get join token for workers] *********************************
    node-02:
    node-02: Saturday 19 June 2021  16:07:10 +0100 (0:00:01.997)       0:04:04.722 *********
    node-02:
    node-02: changed: [node-01 -> 127.0.0.1]
    node-02:
    node-02:
    node-02: TASK [k8s-worker : debug] ******************************************************
    node-02:
    node-02: Saturday 19 June 2021  16:07:10 +0100 (0:00:00.389)       0:04:05.111 *********
    node-02:
    node-02: ok: [node-01 -> 127.0.0.1] => {
    node-02:     "token.stdout": "kubeadm join 10.0.0.9:6443 --token hll4oy.y30vgrsixz88zusv     --discovery-token-ca-cert-hash sha256:8dbc28542c14f704b82eda584ca168ec88605131ca121d9a44415bf4e0b329da "
    node-02: }
    node-02:
    node-02:
    node-02: TASK [k8s-worker : join workers to cluster] ************************************
    node-02:
    node-02: Saturday 19 June 2021  16:07:10 +0100 (0:00:00.022)       0:04:05.133 *********
    node-02:
    node-02: changed: [node-01]
    node-02:
    node-02: changed: [node-02]
    node-02:
    node-02:
    node-02: TASK [k8s-worker : kubectl - wait for nodes] ***********************************
    node-02:
    node-02: Saturday 19 June 2021  16:07:36 +0100 (0:00:25.497)       0:04:30.630 *********
    node-02:
    node-02: FAILED - RETRYING: kubectl - wait for nodes (15 retries left).
    node-02:
    node-02: FAILED - RETRYING: kubectl - wait for nodes (14 retries left).
    node-02:
    node-02: FAILED - RETRYING: kubectl - wait for nodes (13 retries left).
    node-02:
    node-02: FAILED - RETRYING: kubectl - wait for nodes (12 retries left).
    node-02:
    node-02: FAILED - RETRYING: kubectl - wait for nodes (11 retries left).
    node-02:
    node-02: FAILED - RETRYING: kubectl - wait for nodes (10 retries left).
    node-02:
    node-02: FAILED - RETRYING: kubectl - wait for nodes (9 retries left).
    node-02:
    node-02: FAILED - RETRYING: kubectl - wait for nodes (8 retries left).
    node-02:
    node-02: changed: [node-01 -> 127.0.0.1]
    node-02:
    node-02:
    node-02: TASK [k8s-worker : kubectl get nodes] ******************************************
    node-02:
    node-02: Saturday 19 June 2021  16:09:00 +0100 (0:01:24.403)       0:05:55.034 *********
    node-02:
    node-02: changed: [node-01 -> 127.0.0.1]
    node-02:
    node-02:
    node-02: TASK [k8s-worker : debug] ******************************************************
    node-02:
    node-02: Saturday 19 June 2021  16:09:01 +0100 (0:00:00.437)       0:05:55.472 *********
    node-02:
    node-02: ok: [node-01 -> 127.0.0.1] => {
    node-02:     "nodes.stdout_lines": [
    node-02:         "NAME      STATUS   ROLES                  AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME",
    node-02:         "control   Ready    control-plane,master   119s   v1.20.1   10.0.0.9      <none>        Ubuntu 20.04.2 LTS   5.4.0-74-generic   containerd://1.4.4",
    node-02:         "node-01   Ready    <none>                 85s    v1.20.1   10.0.0.10     <none>        Ubuntu 20.04.2 LTS   5.4.0-74-generic   containerd://1.4.4",
    node-02:         "node-02   Ready    <none>                 85s    v1.20.1   10.0.0.11     <none>        Ubuntu 20.04.2 LTS   5.4.0-74-generic   containerd://1.4.4"
    node-02:     ]
    node-02: }
    node-02:
    node-02:
    node-02: PLAY RECAP *********************************************************************
    node-02:
    node-02: control                    : ok=29   changed=22   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    node-02: node-01                    : ok=26   changed=20   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    node-02: node-02                    : ok=21   changed=17   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    node-02:
    node-02:
    node-02: Saturday 19 June 2021  16:09:01 +0100 (0:00:00.017)       0:05:55.489 *********
    node-02: ===============================================================================
    node-02:
    node-02: k8s-worker : kubectl - wait for nodes ---------------------------------- 84.40s
    node-02: k8s-install : install k8s components - kubelet, kubeadm, kubectl 1.20.1-00 -- 60.34s
    node-02: k8s-init : pull k8s images --------------------------------------------- 59.50s
    node-02: k8s-prep : install containerd ------------------------------------------ 57.03s
    node-02: k8s-init : initialise kubernetes cluster ------------------------------- 30.32s
    node-02: k8s-worker : join workers to cluster ----------------------------------- 25.50s
    node-02:
    node-02: k8s-install : add k8s repository --------------------------------------- 12.75s
    node-02: k8s-install : install deps for k8s -------------------------------------- 5.66s
    node-02: Gathering Facts --------------------------------------------------------- 3.24s
    node-02: Gathering Facts --------------------------------------------------------- 2.00s
    node-02: k8s-network : apply calico networking ----------------------------------- 1.81s
    node-02: k8s-install : add k8s repo gpg key -------------------------------------- 1.45s
    node-02: k8s-install : mark hold k8s packages ------------------------------------ 1.18s
    node-02: Gathering Facts --------------------------------------------------------- 1.02s
    node-02: k8s-prep : restart containerd ------------------------------------------- 0.82s
    node-02:
    node-02: k8s-prep : insert containerd modules ------------------------------------ 0.80s
    node-02: k8s-prep : create containerd.conf --------------------------------------- 0.78s
    node-02: k8s-install : create /ect/default/kubelet to setup INTERNAL-IP for a node --- 0.75s
    node-02: k8s-prep : create kubernetes-cri sysctl --------------------------------- 0.68s
    node-02: k8s-prep : generate containerd config ----------------------------------- 0.65s
    node-02:
✔ ~/Projects/vagrant-k8s [failOn1stnode L|●1✚ 4…30⚑ 1]
16:09 $

