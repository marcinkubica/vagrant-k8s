12:17 $ vagrant up && ./vagrant-k8s.sh
Bringing machine 'control' up with 'virtualbox' provider...
Bringing machine 'node-01' up with 'virtualbox' provider...
Bringing machine 'node-02' up with 'virtualbox' provider...
==> control: Importing base box 'ubuntu/focal64'...
==> control: Matching MAC address for NAT networking...
==> control: Checking if box 'ubuntu/focal64' version '20210518.0.0' is up to date...
==> control: Setting the name of the VM: vagrant-k8s_control_1622891895459_76312
==> control: Clearing any previously set network interfaces...
==> control: Preparing network interfaces based on configuration...
    control: Adapter 1: nat
    control: Adapter 2: hostonly
==> control: Forwarding ports...
    control: 22 (guest) => 60200 (host) (adapter 1)
    control: 6443 (guest) => 6443 (host) (adapter 1)
    control: 22 (guest) => 2222 (host) (adapter 1)
==> control: Running 'pre-boot' VM customizations...
==> control: Booting VM...
==> control: Waiting for machine to boot. This may take a few minutes...
    control: SSH address: 127.0.0.1:2222
    control: SSH username: vagrant
    control: SSH auth method: private key
    control: Warning: Connection reset. Retrying...
    control: Warning: Remote connection disconnect. Retrying...
    control:
    control: Vagrant insecure key detected. Vagrant will automatically replace
    control: this with a newly generated keypair for better security.
    control:
    control: Inserting generated public key within guest...
    control: Removing insecure key from the guest if it's present...
    control: Key inserted! Disconnecting and reconnecting using new SSH key...
==> control: Machine booted and ready!
==> control: Checking for guest additions in VM...
==> control: Setting hostname...
==> control: Configuring and enabling network interfaces...
==> control: Mounting shared folders...
    control: /vagrant => /home/booker/Projects/vagrant-k8s
==> node-01: Importing base box 'ubuntu/focal64'...
==> node-01: Matching MAC address for NAT networking...
==> node-01: Checking if box 'ubuntu/focal64' version '20210518.0.0' is up to date...
==> node-01: Setting the name of the VM: vagrant-k8s_node-01_1622891939220_23189
==> node-01: Fixed port collision for 22 => 2222. Now on port 2200.
==> node-01: Clearing any previously set network interfaces...
==> node-01: Preparing network interfaces based on configuration...
    node-01: Adapter 1: nat
    node-01: Adapter 2: hostonly
==> node-01: Forwarding ports...
    node-01: 22 (guest) => 60201 (host) (adapter 1)
    node-01: 22 (guest) => 2200 (host) (adapter 1)
==> node-01: Running 'pre-boot' VM customizations...
==> node-01: Booting VM...
==> node-01: Waiting for machine to boot. This may take a few minutes...
    node-01: SSH address: 127.0.0.1:2200
    node-01: SSH username: vagrant
    node-01: SSH auth method: private key
    node-01: Warning: Connection reset. Retrying...
    node-01: Warning: Remote connection disconnect. Retrying...
    node-01:
    node-01: Vagrant insecure key detected. Vagrant will automatically replace
    node-01: this with a newly generated keypair for better security.
    node-01:
    node-01: Inserting generated public key within guest...
    node-01: Removing insecure key from the guest if it's present...
    node-01: Key inserted! Disconnecting and reconnecting using new SSH key...
==> node-01: Machine booted and ready!
==> node-01: Checking for guest additions in VM...
==> node-01: Setting hostname...
==> node-01: Configuring and enabling network interfaces...
==> node-01: Mounting shared folders...
    node-01: /vagrant => /home/booker/Projects/vagrant-k8s
==> node-02: Importing base box 'ubuntu/focal64'...
==> node-02: Matching MAC address for NAT networking...
==> node-02: Checking if box 'ubuntu/focal64' version '20210518.0.0' is up to date...
==> node-02: Setting the name of the VM: vagrant-k8s_node-02_1622891989272_45124
==> node-02: Fixed port collision for 22 => 2222. Now on port 2201.
==> node-02: Clearing any previously set network interfaces...
==> node-02: Preparing network interfaces based on configuration...
    node-02: Adapter 1: nat
    node-02: Adapter 2: hostonly
==> node-02: Forwarding ports...
    node-02: 22 (guest) => 60202 (host) (adapter 1)
    node-02: 22 (guest) => 2201 (host) (adapter 1)
==> node-02: Running 'pre-boot' VM customizations...
==> node-02: Booting VM...
==> node-02: Waiting for machine to boot. This may take a few minutes...
    node-02: SSH address: 127.0.0.1:2201
    node-02: SSH username: vagrant
    node-02: SSH auth method: private key
    node-02: Warning: Connection reset. Retrying...
    node-02: Warning: Remote connection disconnect. Retrying...
    node-02:
    node-02: Vagrant insecure key detected. Vagrant will automatically replace
    node-02: this with a newly generated keypair for better security.
    node-02:
    node-02: Inserting generated public key within guest...
    node-02: Removing insecure key from the guest if it's present...
    node-02: Key inserted! Disconnecting and reconnecting using new SSH key...
==> node-02: Machine booted and ready!
==> node-02: Checking for guest additions in VM...
==> node-02: Setting hostname...
==> node-02: Configuring and enabling network interfaces...
==> node-02: Mounting shared folders...
    node-02: /vagrant => /home/booker/Projects/vagrant-k8s

PLAY [all] *****************************************************************************************************************************************************************

TASK [Gathering Facts] *****************************************************************************************************************************************************
Saturday 05 June 2021  12:20:20 +0100 (0:00:00.033)       0:00:00.033 *********
ok: [node-01]
ok: [node-02]
ok: [control]

TASK [k8s-prep : create containerd.conf] ***********************************************************************************************************************************
Saturday 05 June 2021  12:20:23 +0100 (0:00:03.092)       0:00:03.126 *********
changed: [control]
changed: [node-02]
changed: [node-01]

TASK [k8s-prep : insert containerd modules] ********************************************************************************************************************************
Saturday 05 June 2021  12:20:24 +0100 (0:00:00.987)       0:00:04.114 *********
changed: [node-01] => (item=overlay)
changed: [control] => (item=overlay)
changed: [node-02] => (item=overlay)
changed: [control] => (item=br_netfilter)
changed: [node-01] => (item=br_netfilter)
changed: [node-02] => (item=br_netfilter)

TASK [k8s-prep : create kubernetes-cri sysctl] *****************************************************************************************************************************
Saturday 05 June 2021  12:20:26 +0100 (0:00:01.626)       0:00:05.741 *********
changed: [control]
changed: [node-01]
changed: [node-02]

TASK [k8s-prep : reload sysctl] ********************************************************************************************************************************************
Saturday 05 June 2021  12:20:27 +0100 (0:00:00.710)       0:00:06.451 *********
changed: [control]
changed: [node-01]
changed: [node-02]

TASK [k8s-prep : install containerd] ***************************************************************************************************************************************
Saturday 05 June 2021  12:20:27 +0100 (0:00:00.452)       0:00:06.904 *********
changed: [node-01]
changed: [control]
changed: [node-02]

TASK [k8s-prep : grab default containerd config] ***************************************************************************************************************************
Saturday 05 June 2021  12:21:29 +0100 (0:01:01.935)       0:01:08.839 *********
changed: [control]
changed: [node-01]
changed: [node-02]

TASK [k8s-prep : create /etc/containerd] ***********************************************************************************************************************************
Saturday 05 June 2021  12:21:29 +0100 (0:00:00.438)       0:01:09.277 *********
changed: [node-02]
changed: [control]
changed: [node-01]

TASK [k8s-prep : generate containerd config] *******************************************************************************************************************************
Saturday 05 June 2021  12:21:30 +0100 (0:00:00.566)       0:01:09.843 *********
changed: [node-02]
changed: [node-01]
changed: [control]

TASK [k8s-prep : restart containerd] ***************************************************************************************************************************************
Saturday 05 June 2021  12:21:31 +0100 (0:00:00.741)       0:01:10.585 *********
changed: [node-02]
changed: [node-01]
changed: [control]

TASK [k8s-prep : check containerd] *****************************************************************************************************************************************
Saturday 05 June 2021  12:21:31 +0100 (0:00:00.839)       0:01:11.425 *********
ok: [control]
ok: [node-01]
ok: [node-02]

TASK [k8s-prep : disable swap] *********************************************************************************************************************************************
Saturday 05 June 2021  12:21:32 +0100 (0:00:00.587)       0:01:12.012 *********
changed: [node-01]
changed: [control]
changed: [node-02]

TASK [k8s-prep : comment out swap entries in /etc/fstab] *******************************************************************************************************************
Saturday 05 June 2021  12:21:32 +0100 (0:00:00.385)       0:01:12.398 *********
ok: [control]
ok: [node-01]
ok: [node-02]

TASK [k8s-install : install deps for k8s] **********************************************************************************************************************************
Saturday 05 June 2021  12:21:33 +0100 (0:00:00.470)       0:01:12.869 *********
changed: [control]
changed: [node-02]
changed: [node-01]

TASK [k8s-install : add k8s repo gpg key] **********************************************************************************************************************************
Saturday 05 June 2021  12:21:39 +0100 (0:00:06.260)       0:01:19.130 *********
changed: [control]
changed: [node-01]
changed: [node-02]

TASK [k8s-install : add k8s repository] ************************************************************************************************************************************
Saturday 05 June 2021  12:21:41 +0100 (0:00:01.554)       0:01:20.684 *********
changed: [node-01]
changed: [node-02]
changed: [control]

TASK [k8s-install : create /ect/default/kubelet to setup INTERNAL-IP for a node] *******************************************************************************************
Saturday 05 June 2021  12:21:52 +0100 (0:00:11.468)       0:01:32.153 *********
changed: [control]
changed: [node-01]
changed: [node-02]

TASK [k8s-install : install k8s components - kubelet, kubeadm, kubectl 1.20.1-00] ******************************************************************************************
Saturday 05 June 2021  12:21:53 +0100 (0:00:00.784)       0:01:32.938 *********
changed: [control]
changed: [node-02]
changed: [node-01]

TASK [k8s-install : mark hold k8s packages] ********************************************************************************************************************************
Saturday 05 June 2021  12:22:51 +0100 (0:00:57.680)       0:02:30.619 *********
changed: [node-01] => (item=kubelet)
changed: [control] => (item=kubelet)
changed: [node-02] => (item=kubelet)
changed: [node-01] => (item=kubeadm)
changed: [node-02] => (item=kubeadm)
changed: [control] => (item=kubeadm)
changed: [node-01] => (item=kubectl)
changed: [node-02] => (item=kubectl)
changed: [control] => (item=kubectl)

PLAY [control] *************************************************************************************************************************************************************

TASK [Gathering Facts] *****************************************************************************************************************************************************
Saturday 05 June 2021  12:22:52 +0100 (0:00:01.250)       0:02:31.869 *********
ok: [control]

TASK [k8s-init : pull k8s images] ******************************************************************************************************************************************
Saturday 05 June 2021  12:22:53 +0100 (0:00:01.027)       0:02:32.897 *********
changed: [control]

TASK [k8s-init : debug] ****************************************************************************************************************************************************
Saturday 05 June 2021  12:23:56 +0100 (0:01:03.479)       0:03:36.376 *********
ok: [control] => {
    "pull": {
        "changed": true,
        "cmd": "kubeadm config images pull",
        "delta": "0:01:02.649613",
        "end": "2021-06-05 11:23:55.154219",
        "failed": false,
        "rc": 0,
        "start": "2021-06-05 11:22:52.504606",
        "stderr": "I0605 11:22:52.936933    5816 version.go:251] remote version is much newer: v1.21.1; falling back to: stable-1.20",
        "stderr_lines": [
            "I0605 11:22:52.936933    5816 version.go:251] remote version is much newer: v1.21.1; falling back to: stable-1.20"
        ],
        "stdout": "[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.20.7\n[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.20.7\n[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.20.7\n[config/images] Pulled k8s.gcr.io/kube-proxy:v1.20.7\n[config/images] Pulled k8s.gcr.io/pause:3.2\n[config/images] Pulled k8s.gcr.io/etcd:3.4.13-0\n[config/images] Pulled k8s.gcr.io/coredns:1.7.0",
        "stdout_lines": [
            "[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.20.7",
            "[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.20.7",
            "[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.20.7",
            "[config/images] Pulled k8s.gcr.io/kube-proxy:v1.20.7",
            "[config/images] Pulled k8s.gcr.io/pause:3.2",
            "[config/images] Pulled k8s.gcr.io/etcd:3.4.13-0",
            "[config/images] Pulled k8s.gcr.io/coredns:1.7.0"
        ]
    }
}

TASK [k8s-init : initialise kubernetes cluster] ****************************************************************************************************************************
Saturday 05 June 2021  12:23:56 +0100 (0:00:00.033)       0:03:36.409 *********
changed: [control]

TASK [k8s-init : debug] ****************************************************************************************************************************************************
Saturday 05 June 2021  12:24:32 +0100 (0:00:35.811)       0:04:12.221 *********
ok: [control] => {
    "init": {
        "changed": true,
        "cmd": "kubeadm init --apiserver-advertise-address 10.0.0.9 --pod-network-cidr 192.168.0.0/16",
        "delta": "0:00:35.451203",
        "end": "2021-06-05 11:24:31.551750",
        "failed": false,
        "rc": 0,
        "start": "2021-06-05 11:23:56.100547",
        "stderr": "I0605 11:23:56.583628    6078 version.go:251] remote version is much newer: v1.21.1; falling back to: stable-1.20",
        "stderr_lines": [
            "I0605 11:23:56.583628    6078 version.go:251] remote version is much newer: v1.21.1; falling back to: stable-1.20"
        ],
        "stdout": "[init] Using Kubernetes version: v1.20.7\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [control kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.0.0.9]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [control localhost] and IPs [10.0.0.9 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [control localhost] and IPs [10.0.0.9 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\n[apiclient] All control plane components are healthy after 26.503674 seconds\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config-1.20\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Skipping phase. Please see --upload-certs\n[mark-control-plane] Marking the node control as control-plane by adding the labels \"node-role.kubernetes.io/master=''\" and \"node-role.kubernetes.io/control-plane='' (deprecated)\"\n[mark-control-plane] Marking the node control as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n[bootstrap-token] Using token: qtr1i3.dg2013ws0vhsz854\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 10.0.0.9:6443 --token qtr1i3.dg2013ws0vhsz854 \\\n    --discovery-token-ca-cert-hash sha256:32f406dba19a4dcfe05eed2328c2f69826a63519e85e823278fa6a2857544bdd ",
        "stdout_lines": [
            "[init] Using Kubernetes version: v1.20.7",
            "[preflight] Running pre-flight checks",
            "[preflight] Pulling images required for setting up a Kubernetes cluster",
            "[preflight] This might take a minute or two, depending on the speed of your internet connection",
            "[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'",
            "[certs] Using certificateDir folder \"/etc/kubernetes/pki\"",
            "[certs] Generating \"ca\" certificate and key",
            "[certs] Generating \"apiserver\" certificate and key",
            "[certs] apiserver serving cert is signed for DNS names [control kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.0.0.9]",
            "[certs] Generating \"apiserver-kubelet-client\" certificate and key",
            "[certs] Generating \"front-proxy-ca\" certificate and key",
            "[certs] Generating \"front-proxy-client\" certificate and key",
            "[certs] Generating \"etcd/ca\" certificate and key",
            "[certs] Generating \"etcd/server\" certificate and key",
            "[certs] etcd/server serving cert is signed for DNS names [control localhost] and IPs [10.0.0.9 127.0.0.1 ::1]",
            "[certs] Generating \"etcd/peer\" certificate and key",
            "[certs] etcd/peer serving cert is signed for DNS names [control localhost] and IPs [10.0.0.9 127.0.0.1 ::1]",
            "[certs] Generating \"etcd/healthcheck-client\" certificate and key",
            "[certs] Generating \"apiserver-etcd-client\" certificate and key",
            "[certs] Generating \"sa\" key and public key",
            "[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"",
            "[kubeconfig] Writing \"admin.conf\" kubeconfig file",
            "[kubeconfig] Writing \"kubelet.conf\" kubeconfig file",
            "[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file",
            "[kubeconfig] Writing \"scheduler.conf\" kubeconfig file",
            "[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"",
            "[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"",
            "[kubelet-start] Starting the kubelet",
            "[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"",
            "[control-plane] Creating static Pod manifest for \"kube-apiserver\"",
            "[control-plane] Creating static Pod manifest for \"kube-controller-manager\"",
            "[control-plane] Creating static Pod manifest for \"kube-scheduler\"",
            "[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"",
            "[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s",
            "[apiclient] All control plane components are healthy after 26.503674 seconds",
            "[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace",
            "[kubelet] Creating a ConfigMap \"kubelet-config-1.20\" in namespace kube-system with the configuration for the kubelets in the cluster",
            "[upload-certs] Skipping phase. Please see --upload-certs",
            "[mark-control-plane] Marking the node control as control-plane by adding the labels \"node-role.kubernetes.io/master=''\" and \"node-role.kubernetes.io/control-plane='' (deprecated)\"",
            "[mark-control-plane] Marking the node control as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]",
            "[bootstrap-token] Using token: qtr1i3.dg2013ws0vhsz854",
            "[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles",
            "[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes",
            "[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials",
            "[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token",
            "[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster",
            "[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace",
            "[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key",
            "[addons] Applied essential addon: CoreDNS",
            "[addons] Applied essential addon: kube-proxy",
            "",
            "Your Kubernetes control-plane has initialized successfully!",
            "",
            "To start using your cluster, you need to run the following as a regular user:",
            "",
            "  mkdir -p $HOME/.kube",
            "  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config",
            "  sudo chown $(id -u):$(id -g) $HOME/.kube/config",
            "",
            "Alternatively, if you are the root user, you can run:",
            "",
            "  export KUBECONFIG=/etc/kubernetes/admin.conf",
            "",
            "You should now deploy a pod network to the cluster.",
            "Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:",
            "  https://kubernetes.io/docs/concepts/cluster-administration/addons/",
            "",
            "Then you can join any number of worker nodes by running the following on each as root:",
            "",
            "kubeadm join 10.0.0.9:6443 --token qtr1i3.dg2013ws0vhsz854 \\",
            "    --discovery-token-ca-cert-hash sha256:32f406dba19a4dcfe05eed2328c2f69826a63519e85e823278fa6a2857544bdd "
        ]
    }
}

TASK [k8s-init : setup vagrant user for kube config | directory] ***********************************************************************************************************
Saturday 05 June 2021  12:24:32 +0100 (0:00:00.026)       0:04:12.247 *********
changed: [control]

TASK [k8s-init : setup vagrant user for kube config | copy admin conf] *****************************************************************************************************
Saturday 05 June 2021  12:24:33 +0100 (0:00:00.368)       0:04:12.616 *********
changed: [control]

TASK [k8s-network : apply calico networking] *******************************************************************************************************************************
Saturday 05 June 2021  12:24:33 +0100 (0:00:00.331)       0:04:12.947 *********
changed: [control]

TASK [k8s-network : debug] *************************************************************************************************************************************************
Saturday 05 June 2021  12:24:35 +0100 (0:00:01.598)       0:04:14.545 *********
ok: [control] => {
    "c": {
        "changed": true,
        "cmd": "kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml",
        "delta": "0:00:01.295542",
        "end": "2021-06-05 11:24:33.893152",
        "failed": false,
        "rc": 0,
        "start": "2021-06-05 11:24:32.597610",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "configmap/calico-config created\ncustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\nclusterrole.rbac.authorization.k8s.io/calico-kube-controllers created\nclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created\nclusterrole.rbac.authorization.k8s.io/calico-node created\nclusterrolebinding.rbac.authorization.k8s.io/calico-node created\ndaemonset.apps/calico-node created\nserviceaccount/calico-node created\ndeployment.apps/calico-kube-controllers created\nserviceaccount/calico-kube-controllers created\npoddisruptionbudget.policy/calico-kube-controllers created",
        "stdout_lines": [
            "configmap/calico-config created",
            "customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created",
            "customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created",
            "clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created",
            "clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created",
            "clusterrole.rbac.authorization.k8s.io/calico-node created",
            "clusterrolebinding.rbac.authorization.k8s.io/calico-node created",
            "daemonset.apps/calico-node created",
            "serviceaccount/calico-node created",
            "deployment.apps/calico-kube-controllers created",
            "serviceaccount/calico-kube-controllers created",
            "poddisruptionbudget.policy/calico-kube-controllers created"
        ]
    }
}

TASK [k8s-network : check for all pods running] ****************************************************************************************************************************
Saturday 05 June 2021  12:24:35 +0100 (0:00:00.026)       0:04:14.571 *********
changed: [control]

PLAY [workers] *************************************************************************************************************************************************************

TASK [Gathering Facts] *****************************************************************************************************************************************************
Saturday 05 June 2021  12:24:35 +0100 (0:00:00.538)       0:04:15.110 *********
ok: [node-01]
ok: [node-02]

TASK [k8s-worker : get join token for workers] *****************************************************************************************************************************
Saturday 05 June 2021  12:24:38 +0100 (0:00:03.204)       0:04:18.314 *********
changed: [node-01 -> 127.0.0.1]
changed: [node-02 -> 127.0.0.1]

TASK [k8s-worker : debug] **************************************************************************************************************************************************
Saturday 05 June 2021  12:24:39 +0100 (0:00:00.467)       0:04:18.782 *********
ok: [node-01 -> 127.0.0.1] => {
    "token.stdout": "kubeadm join 10.0.0.9:6443 --token ov45sy.4nno4w5hxaytv0n8     --discovery-token-ca-cert-hash sha256:32f406dba19a4dcfe05eed2328c2f69826a63519e85e823278fa6a2857544bdd "
}
ok: [node-02 -> 127.0.0.1] => {
    "token.stdout": "kubeadm join 10.0.0.9:6443 --token j4albu.0sdm25tlxlb9xkm9     --discovery-token-ca-cert-hash sha256:32f406dba19a4dcfe05eed2328c2f69826a63519e85e823278fa6a2857544bdd "
}

TASK [k8s-worker : join workers to cluster] ********************************************************************************************************************************
Saturday 05 June 2021  12:24:39 +0100 (0:00:00.040)       0:04:18.822 *********
changed: [node-02]
changed: [node-01]

TASK [k8s-worker : kubectl - wait for nodes] *******************************************************************************************************************************
Saturday 05 June 2021  12:25:08 +0100 (0:00:28.972)       0:04:47.795 *********
FAILED - RETRYING: kubectl - wait for nodes (10 retries left).
FAILED - RETRYING: kubectl - wait for nodes (9 retries left).
FAILED - RETRYING: kubectl - wait for nodes (8 retries left).
FAILED - RETRYING: kubectl - wait for nodes (7 retries left).
FAILED - RETRYING: kubectl - wait for nodes (6 retries left).
FAILED - RETRYING: kubectl - wait for nodes (5 retries left).
FAILED - RETRYING: kubectl - wait for nodes (4 retries left).
changed: [node-01 -> 127.0.0.1]

TASK [k8s-worker : kubectl get nodes] **************************************************************************************************************************************
Saturday 05 June 2021  12:26:26 +0100 (0:01:17.992)       0:06:05.788 *********
changed: [node-01 -> 127.0.0.1]

TASK [k8s-worker : debug] **************************************************************************************************************************************************
Saturday 05 June 2021  12:26:26 +0100 (0:00:00.518)       0:06:06.307 *********
ok: [node-01 -> 127.0.0.1] => {
    "nodes.stdout_lines": [
        "NAME      STATUS   ROLES                  AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME",
        "control   Ready    control-plane,master   117s   v1.20.1   10.0.0.9      <none>        Ubuntu 20.04.2 LTS   5.4.0-73-generic   containerd://1.3.3-0ubuntu2.3",
        "node-01   Ready    <none>                 82s    v1.20.1   10.0.0.10     <none>        Ubuntu 20.04.2 LTS   5.4.0-73-generic   containerd://1.3.3-0ubuntu2.3",
        "node-02   Ready    <none>                 82s    v1.20.1   10.0.0.11     <none>        Ubuntu 20.04.2 LTS   5.4.0-73-generic   containerd://1.3.3-0ubuntu2.3"
    ]
}

PLAY RECAP *****************************************************************************************************************************************************************
control                    : ok=29   changed=22   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
node-01                    : ok=26   changed=20   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
node-02                    : ok=23   changed=18   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0

Saturday 05 June 2021  12:26:26 +0100 (0:00:00.021)       0:06:06.328 *********
===============================================================================
k8s-worker : kubectl - wait for nodes ------------------------------------------------------------------------------------------------------------------------------ 77.99s
k8s-init : pull k8s images ----------------------------------------------------------------------------------------------------------------------------------------- 63.48s
k8s-prep : install containerd -------------------------------------------------------------------------------------------------------------------------------------- 61.94s
k8s-install : install k8s components - kubelet, kubeadm, kubectl 1.20.1-00 ----------------------------------------------------------------------------------------- 57.68s
k8s-init : initialise kubernetes cluster --------------------------------------------------------------------------------------------------------------------------- 35.81s
k8s-worker : join workers to cluster ------------------------------------------------------------------------------------------------------------------------------- 28.97s
k8s-install : add k8s repository ----------------------------------------------------------------------------------------------------------------------------------- 11.47s
k8s-install : install deps for k8s ---------------------------------------------------------------------------------------------------------------------------------- 6.26s
Gathering Facts ----------------------------------------------------------------------------------------------------------------------------------------------------- 3.20s
Gathering Facts ----------------------------------------------------------------------------------------------------------------------------------------------------- 3.09s
k8s-prep : insert containerd modules -------------------------------------------------------------------------------------------------------------------------------- 1.63s
k8s-network : apply calico networking ------------------------------------------------------------------------------------------------------------------------------- 1.60s
k8s-install : add k8s repo gpg key ---------------------------------------------------------------------------------------------------------------------------------- 1.55s
k8s-install : mark hold k8s packages -------------------------------------------------------------------------------------------------------------------------------- 1.25s
Gathering Facts ----------------------------------------------------------------------------------------------------------------------------------------------------- 1.03s
k8s-prep : create containerd.conf ----------------------------------------------------------------------------------------------------------------------------------- 0.99s
k8s-prep : restart containerd --------------------------------------------------------------------------------------------------------------------------------------- 0.84s
k8s-install : create /ect/default/kubelet to setup INTERNAL-IP for a node ------------------------------------------------------------------------------------------- 0.78s
k8s-prep : generate containerd config ------------------------------------------------------------------------------------------------------------------------------- 0.74s
k8s-prep : create kubernetes-cri sysctl ----------------------------------------------------------------------------------------------------------------------------- 0.71s

